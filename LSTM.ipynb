{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM for NLP\n",
    "- predict next word in Anna Karenina\n",
    "- first with pytorch lstm\n",
    "- then with numpy lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys \n",
    "import regex as re\n",
    "from typing import Tuple, Dict, List\n",
    "from collections import Counter\n",
    "\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in raw text\n",
    "sys.path.append('../../data/')\n",
    "with open('data/anna.txt' , 'r') as f:\n",
    "    anna = f.read()\n",
    "print(type(anna))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up line breaks and add space around punctuation (for tokenization)\n",
    "clean_text=anna.lower().replace(\"\\n\", \" \") \n",
    "clean_text=clean_text.replace(\"-\", \" \") \n",
    "for x in \",.:;?!$()/_&%*@'`\":\n",
    "    clean_text=clean_text.replace(f\"{x}\", f\" {x} \")\n",
    "    clean_text=clean_text.replace('\"', ' \" ') \n",
    "text=clean_text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## implement simplified word tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocab\n",
    "word_counts = Counter(text)\n",
    "vocab = sorted(word_counts, key=word_counts.get,\n",
    "reverse = True)\n",
    "print(vocab[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definde tokenizer-encoder and tokenizer-decoder\n",
    "encoder={v:k for k,v in enumerate(vocab)} \n",
    "decoder={k:v for k,v in enumerate(vocab)}\n",
    "\n",
    "word_tokenizer = {'encoder': encoder, 'decoder': decoder}\n",
    "# save the simple tokenizer\n",
    "import joblib\n",
    "joblib.dump(word_tokenizer, 'word_tokenizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply tokenizer\n",
    "tokens = [encoder[x] for x in text]\n",
    "print(text[:20])\n",
    "print(tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train, test sets\n",
    "len_of_text = len(text)\n",
    "print(len(tokens))\n",
    "split_point = int(np.round(len_of_text*0.7))\n",
    "train_x = tokens[:split_point]\n",
    "print(len(train_x))\n",
    "test_x = tokens[split_point:]\n",
    "print(len(test_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creat PyTorch dataset \n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# 1. Subclass torch.utils.data.Dataset\n",
    "class my_dataset(Dataset):\n",
    "    \n",
    "    # 2. Initialize with customized varargs \n",
    "    def __init__(self, tokenized_text: str, max_length: int, stride: int=1) -> None:\n",
    "        \n",
    "        # 3. Create class attributes\n",
    "        # initialize input_tokens_x and target_tokens_y\n",
    "        self.input_tokens_x = [] # alternative nested tensor \n",
    "        self.target_tokens_y = []\n",
    "        # set y as stride number of tokens trailing x \n",
    "        for i in range(0, (len(tokenized_text)-max_length), stride):\n",
    "            x_tmp = tokenized_text[i : (i+max_length)]\n",
    "            y_tmp = tokenized_text[(i+1) : (i+max_length+1)]\n",
    "            self.input_tokens_x.append(torch.tensor(x_tmp))\n",
    "            self.target_tokens_y.append(torch.tensor(y_tmp))\n",
    "    \n",
    "    # 4. Overwrite the __len__() method to return number of rows in the dataset\n",
    "    def __len__(self) -> int:\n",
    "        \"Returns the number of rows / pairs of x-y sequences in the dataset\"\n",
    "        return len(self.input_tokens_x)\n",
    "    \n",
    "    # 5. Overwrite the __getitem__() method (required for subclasses of torch.utils.data.Dataset)\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:\n",
    "        \"Returns one sample of data, data and label (X, y).\"\n",
    "        return self.input_tokens_x[idx], self.target_tokens_y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and save torch dataset for later use\n",
    "context_length = 100\n",
    "dataset_train = my_dataset(tokenized_text=train_x, max_length=context_length, stride=1)\n",
    "torch.save(dataset_train, 'data/train.pt')\n",
    "dataset_test = my_dataset(tokenized_text=test_x, max_length=context_length, stride=1)\n",
    "torch.save(dataset_test, 'data/test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data with torch dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "torch.manual_seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "# num_workers = 3 \n",
    "# num_workers=num_workers # number of cpu processes to use for preprocessing\n",
    "# num_works makes next(iter()) stuck in a loop\n",
    "\n",
    "loader_train = DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True, # if True, # drops the last batch if it is shorter than the specified batch_size to prevent loss spikes during training  \n",
    ")\n",
    "\n",
    "print(type(loader_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the next pair in dataset\n",
    "x,y=next(iter(loader_train))\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(encoder) # len(vocab) # \n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class word_lstm(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size, \n",
    "                 emb_dim,\n",
    "                 lstm_layers,\n",
    "                 drop_out_rate):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_dim = emb_dim\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.drop_out_rate = drop_out_rate\n",
    "\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.emb_dim)\n",
    "        self.lstm = nn.LSTM(input_size=self.emb_dim,\n",
    "                            hidden_size=self.emb_dim,\n",
    "                            num_layers=self.lstm_layers,\n",
    "                            dropout=self.drop_out_rate,\n",
    "                            batch_first=True)\n",
    "        self.fc = nn.Linear(self.emb_dim, self.vocab_size, bias=True)\n",
    "\n",
    "    def forward(self, x, hc):\n",
    "        embed = self.embedding(x)\n",
    "        x, hc = self.lstm(embed, hc)\n",
    "        x = self.fc(x)\n",
    "        return x, hc\n",
    "    \n",
    "    def init_hidden(self, context_length):\n",
    "        weight = next(self.parameters()).data\n",
    "        return (weight.new(self.lstm_layers, context_length, self.emb_dim).zero_(),\n",
    "                weight.new(self.lstm_layers, context_length, self.emb_dim).zero_())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word_lstm(vocab_size=vocab_size,\n",
    "                  emb_dim=128,\n",
    "                  lstm_layers=3,\n",
    "                  drop_out_rate=0.2).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "for epoch in range(50):\n",
    "    tloss=0\n",
    "    sh,sc = model.init_hidden(batch_size)\n",
    "    for i, (x,y) in enumerate(loader_train):    \n",
    "        if x.shape[0]==batch_size:\n",
    "            inputs, targets = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output, (sh,sc) = model(inputs, (sh,sc))\n",
    "            loss = loss_func(output.transpose(1,2),targets)\n",
    "            sh,sc=sh.detach(),sc.detach()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "            optimizer.step()\n",
    "            tloss+=loss.item()\n",
    "        if (i+1)%100==0:\n",
    "            print(f\"at epoch {epoch} iteration {i+1}\\\n",
    "            average loss = {tloss/(i+1)}\")\n",
    "\n",
    "# at epoch 0 iteration 100            average loss = 6.056614637374878\n",
    "# at epoch 0 iteration 500            average loss = 6.038397843360901\n",
    "# at epoch 0 iteration 1200            average loss = 6.0286973142623905"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\"model/word_lstm).pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference - greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference - top-k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## numpy LSTM\n",
    "- 3 gates\n",
    "    - input gate i = sigmoid(W_i[h_(t-1), x_t])\n",
    "    - forget gate f = sigmoid(W_f[h_(t-1), x_t])\n",
    "    - output gate o = sigmoid(W_o[h_(t-1), x_t])\n",
    "- a candidate g = tahn(W_g[h_(t-1), x_t])\n",
    "- a cell's memory c_t = element_wise_multiplication(c_(t-1), f) + element_wise_multiplication(g, i)\n",
    "- current hidden state h_t = element_wise_multiplication(tahn(c_t), o)\n",
    "- NOTE: c_t is not directly used to calculate current step's output but sent to next time step (memory used in the future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize lstm \n",
    "class my_lstm:\n",
    "    def __init__(self, d_hidden, vocab_size):\n",
    "        self.d_hidden = d_hidden\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_hv = d_hidden + d_hidden\n",
    "    \n",
    "    def init_orthogonal(weight_mat):\n",
    "        \"\"\"\n",
    "        orthogonalize (QR decomp) weight matrix\n",
    "        see https://arxiv.org/abs/1312.6120\n",
    "        \"\"\"\n",
    "        if weight_mat.ndim < 2:\n",
    "            raise ValueError(\"Only parameters with 2 or more dimensions are supported.\")\n",
    "\n",
    "        rows, cols = weight_mat.shape\n",
    "        \n",
    "        new_param = np.random.randn(rows, cols)\n",
    "        \n",
    "        if rows < cols:\n",
    "            new_param = new_param.T\n",
    "        \n",
    "        # Compute QR factorization\n",
    "        q, r = np.linalg.qr(new_param)\n",
    "        \n",
    "        # Make Q uniform according to https://arxiv.org/pdf/math-ph/0609050.pdf\n",
    "        d = np.diag(r, 0)\n",
    "        ph = np.sign(d)\n",
    "        q *= ph\n",
    "\n",
    "        if rows < cols:\n",
    "            q = q.T\n",
    "        \n",
    "        new_param = q\n",
    "        \n",
    "        return new_param\n",
    "    \n",
    "    def params(self, init_orthogonal=init_orthogonal):\n",
    "        # initialize weights\n",
    "        # input gate\n",
    "        W_i = np.random.randn(self.d_hidden, self.d_hv)\n",
    "        # forget gate\n",
    "        W_f = np.random.randn(self.d_hidden, self.d_hv)\n",
    "        # output gate\n",
    "        W_o = np.random.randn(self.d_hidden, self.d_hv)\n",
    "        # candidate\n",
    "        W_g = np.random.randn(self.d_hidden, self.d_hv)\n",
    "        # outout (for the entire model)\n",
    "        W_v = np.random.randn(self.vocab_size, self.d_hidden)\n",
    "\n",
    "        # orthoganalize (see https://arxiv.org/abs/1312.6120)\n",
    "        W_f = init_orthogonal(W_f)\n",
    "        W_i = init_orthogonal(W_i)\n",
    "        W_g = init_orthogonal(W_g)\n",
    "        W_o = init_orthogonal(W_o)\n",
    "        W_v = init_orthogonal(W_v)\n",
    "        \n",
    "\n",
    "        # initialize biases\n",
    "        # input gate\n",
    "        b_i = np.zeros((self.d_hidden, 1))\n",
    "        # forget gate\n",
    "        b_f = np.zeros((self.d_hidden, 1))\n",
    "        # output gate\n",
    "        b_o = np.zeros((self.d_hidden, 1))\n",
    "        # candidate\n",
    "        b_g = np.zeros((self.d_hidden, 1))\n",
    "        # outout (for the entire model)\n",
    "        b_v = np.zeros((self.vocab_size, 1))\n",
    "        \n",
    "        return W_f, W_i, W_g, W_o, W_v, b_f, b_i, b_g, b_o, b_v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d_hidden = 128\n",
    "init_lstm = my_lstm(d_hidden=d_hidden, vocab_size=vocab_size)\n",
    "param = init_lstm.params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, derivative=False):\n",
    "    \"\"\"\n",
    "    Computes the element-wise sigmoid activation function for an array x.\n",
    "\n",
    "    Args:\n",
    "     `x`: the array where the function is applied\n",
    "     `derivative`: if set to True will return the derivative instead of the forward pass\n",
    "    \"\"\"\n",
    "    x_safe = x + 1e-12\n",
    "    f = 1 / (1 + np.exp(-x_safe))\n",
    "    \n",
    "    if derivative: # Return the derivative of the function evaluated at x\n",
    "        return f * (1 - f)\n",
    "    else: # Return the forward pass of the function at x\n",
    "        return f\n",
    "    \n",
    "def softmax(x, derivative=False):\n",
    "    \"\"\"\n",
    "    Computes the softmax for an array x.\n",
    "    \n",
    "    Args:\n",
    "     `x`: the array where the function is applied\n",
    "     `derivative`: if set to True will return the derivative instead of the forward pass\n",
    "    \"\"\"\n",
    "    x_safe = x + 1e-12\n",
    "    f = np.exp(x_safe) / np.sum(np.exp(x_safe))\n",
    "    \n",
    "    if derivative: # Return the derivative of the function evaluated at x\n",
    "        pass # We will not need this one\n",
    "    else: # Return the forward pass of the function at x\n",
    "        return f\n",
    "    \n",
    "def tanh(x, derivative=False):\n",
    "    \"\"\"\n",
    "    Computes the element-wise tanh activation function for an array x.\n",
    "\n",
    "    Args:\n",
    "     `x`: the array where the function is applied\n",
    "     `derivative`: if set to True will return the derivative instead of the forward pass\n",
    "    \"\"\"\n",
    "    x_safe = x + 1e-12\n",
    "    f = (np.exp(x_safe)-np.exp(-x_safe))/(np.exp(x_safe)+np.exp(-x_safe))\n",
    "    \n",
    "    if derivative: # Return the derivative of the function evaluated at x\n",
    "        return 1-f**2\n",
    "    else: # Return the forward pass of the function at x\n",
    "        return f\n",
    "    \n",
    "def clip_gradient_norm(grads, max_norm=0.25):\n",
    "    \"\"\"\n",
    "    Clips gradients to have a maximum norm of `max_norm`.\n",
    "    This is to prevent the exploding gradients problem.\n",
    "    \"\"\" \n",
    "    # Set the maximum of the norm to be of type float\n",
    "    max_norm = float(max_norm)\n",
    "    total_norm = 0\n",
    "    \n",
    "    # Calculate the L2 norm squared for each gradient and add them to the total norm\n",
    "    for grad in grads:\n",
    "        grad_norm = np.sum(np.power(grad, 2))\n",
    "        total_norm += grad_norm\n",
    "    \n",
    "    total_norm = np.sqrt(total_norm)\n",
    "    \n",
    "    # Calculate clipping coeficient\n",
    "    clip_coef = max_norm / (total_norm + 1e-6)\n",
    "    \n",
    "    # If the total norm is larger than the maximum allowable norm, then clip the gradient\n",
    "    if clip_coef < 1:\n",
    "        for grad in grads:\n",
    "            grad *= clip_coef\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "def forward(inputs, d_hidden, h_prev, C_prev, p=param):\n",
    "    \"\"\"\n",
    "    args in:\n",
    "    inputs: x_t of shape (n_x, m).\n",
    "    h_prev: h_(t-1) of shape (n_a, m)\n",
    "    C_prev: c_(t-1) of shape (n_a, m)\n",
    "    p: list of params from my_lstm:\n",
    "        W_f: (n_a, n_a + n_x)\n",
    "        b_f: n_a, 1)\n",
    "        W_i: (n_a, n_a + n_x)\n",
    "        b_i: (n_a, 1)\n",
    "        W_g: (n_a, n_a + n_x)\n",
    "        b_g: (n_a, 1)\n",
    "        W_o: (n_a, n_a + n_x)\n",
    "        b_o: (n_a, 1)\n",
    "        W_v: (n_v, n_a)\n",
    "        b_v: n_v, 1)\n",
    "    Returns:\n",
    "    z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s -- lists of size m containing the computations in each forward pass\n",
    "    outputs -- prediction at timestep \"t\", numpy array of shape (n_v, m)\n",
    "    \"\"\"\n",
    "    assert h_prev.shape == (d_hidden, 1)\n",
    "    assert C_prev.shape == (d_hidden, 1)\n",
    "\n",
    "    # unpack params\n",
    "    W_f, W_i, W_g, W_o, W_v, b_f, b_i, b_g, b_o, b_v = p\n",
    "    \n",
    "    # Save a list of computations for each of the components in the LSTM\n",
    "    z_s, f_s, i_s,  = [], [] ,[], []\n",
    "    g_s, C_s, o_s, h_s = [], [] ,[], []\n",
    "    v_s, output_s =  [], [] \n",
    "    \n",
    "    # Append the initial cell and hidden state to their respective lists\n",
    "    h_s.append(h_prev)\n",
    "    C_s.append(C_prev)\n",
    "    \n",
    "    for x in inputs:\n",
    "        \n",
    "        # stack input and hidden state\n",
    "        z = np.row_stack((h_prev, x))\n",
    "        z_s.append(z)\n",
    "        \n",
    "        # forget gate\n",
    "        f = sigmoid(np.dot(W_f, z) + b_f)\n",
    "        f_s.append(f)\n",
    "        \n",
    "        # input gate\n",
    "        i = sigmoid(np.dot(W_i, z) + b_i)\n",
    "        i_s.append(i)\n",
    "        \n",
    "        # candidate\n",
    "        g = tanh(np.dot(W_g, z) + b_g)\n",
    "        g_s.append(g)\n",
    "        \n",
    "        # memory state\n",
    "        C_prev = f * C_prev + i * g \n",
    "        C_s.append(C_prev)\n",
    "        \n",
    "        # output gate\n",
    "        o = sigmoid(np.dot(W_o, z) + b_o)\n",
    "        o_s.append(o)\n",
    "        \n",
    "        # hidden state\n",
    "        h_prev = o * tanh(C_prev)\n",
    "        h_s.append(h_prev)\n",
    "\n",
    "        # logits / model output layer\n",
    "        v = np.dot(W_v, h_prev) + b_v\n",
    "        v_s.append(v)\n",
    "        \n",
    "        # softmax the output logits\n",
    "        output = softmax(v)\n",
    "        output_s.append(output)\n",
    "\n",
    "    return z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, output_s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize hidden state as zeros\n",
    "h = np.zeros((d_hidden, 1))\n",
    "c = np.zeros((d_hidden, 1))\n",
    "\n",
    "# Forward pass\n",
    "z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs = forward(param, h, c, param)\n",
    "\n",
    "output_sentence = [word_tokenizer['decoder'][np.argmax(output)] for output in outputs]\n",
    "print('Input sentence:')\n",
    "print(inputs)\n",
    "\n",
    "print('\\nTarget sequence:')\n",
    "print(targets)\n",
    "\n",
    "print('\\nPredicted sequence:')\n",
    "print([word_tokenizer['decoder'][np.argmax(output)] for output in outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward pass\n",
    "def backward(z, f, i, g, C, o, h, v, outputs, targets, p=param):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    z: list of np.row_stack((h_prev, x)) of size m.\n",
    "    f: list of forget gate computations of size m.\n",
    "    i: list of input gate computations of size m.\n",
    "    g: list of candidate computations of size m.\n",
    "    C: list of Cell states of size m+1.\n",
    "    o: list of output gate computations of size m.\n",
    "    h: list of Hidden state computations of size m+1.\n",
    "    v: list of logit computations of size m.\n",
    "    outputs: list ofoutputs of size m.\n",
    "    targets: list oftargets of size m.\n",
    "    p: tuple of params\n",
    "    Returns:\n",
    "    loss -- crossentropy loss for all elements in output\n",
    "    grads -- lists of gradients of every element in p\n",
    "    \"\"\"\n",
    "\n",
    "    # unpack parameters\n",
    "    W_f, W_i, W_g, W_o, W_v, b_f, b_i, b_g, b_o, b_v = p\n",
    "\n",
    "    # Initialize gradients as zero\n",
    "    W_f_d = np.zeros_like(W_f)\n",
    "    b_f_d = np.zeros_like(b_f)\n",
    "\n",
    "    W_i_d = np.zeros_like(W_i)\n",
    "    b_i_d = np.zeros_like(b_i)\n",
    "\n",
    "    W_g_d = np.zeros_like(W_g)\n",
    "    b_g_d = np.zeros_like(b_g)\n",
    "\n",
    "    W_o_d = np.zeros_like(W_o)\n",
    "    b_o_d = np.zeros_like(b_o)\n",
    "\n",
    "    W_v_d = np.zeros_like(W_v)\n",
    "    b_v_d = np.zeros_like(b_v)\n",
    "    \n",
    "    # Set the next cell and hidden state equal to zero\n",
    "    dh_next = np.zeros_like(h[0])\n",
    "    dC_next = np.zeros_like(C[0])\n",
    "        \n",
    "    # Track loss\n",
    "    loss = 0\n",
    "    \n",
    "    for t in reversed(range(len(outputs))):\n",
    "        \n",
    "        # Compute the cross entropy\n",
    "        loss += -np.mean(np.log(outputs[t]) * targets[t])\n",
    "        # Get the previous hidden cell state\n",
    "        C_prev= C[t-1]\n",
    "        \n",
    "        # Compute the derivative of the relation of the hidden state to the output layer\n",
    "        dv = np.copy(outputs[t])\n",
    "        dv[np.argmax(targets[t])] -= 1\n",
    "\n",
    "        # Update the gradient of the relation of the hiddenstate to the output layer\n",
    "        W_v_d += np.dot(dv, h[t].T)\n",
    "        b_v_d += dv\n",
    "\n",
    "        # Compute the derivative of the hidden state and output gate\n",
    "        dh = np.dot(W_v.T, dv)        \n",
    "        dh += dh_next\n",
    "        do = dh * tanh(C[t])\n",
    "        do = sigmoid(o[t], derivative=True)*do\n",
    "        \n",
    "        # Update the gradients with respect to the output gate\n",
    "        W_o_d += np.dot(do, z[t].T)\n",
    "        b_o_d += do\n",
    "\n",
    "        # Compute the derivative of the cell state and candidate g\n",
    "        dC = np.copy(dC_next)\n",
    "        dC += dh * o[t] * tanh(tanh(C[t]), derivative=True)\n",
    "        dg = dC * i[t]\n",
    "        dg = tanh(g[t], derivative=True) * dg\n",
    "        \n",
    "        # Update the gradients with respect to the candidate\n",
    "        W_g_d += np.dot(dg, z[t].T)\n",
    "        b_g_d += dg\n",
    "\n",
    "        # Compute the derivative of the input gate and update its gradients\n",
    "        di = dC * g[t]\n",
    "        di = sigmoid(i[t], True) * di\n",
    "        W_i_d += np.dot(di, z[t].T)\n",
    "        b_i_d += di\n",
    "\n",
    "        # Compute the derivative of the forget gate and update its gradients\n",
    "        df = dC * C_prev\n",
    "        df = sigmoid(f[t]) * df\n",
    "        W_f_d += np.dot(df, z[t].T)\n",
    "        b_f_d += df\n",
    "\n",
    "        # Compute the derivative of the input and update the gradients of the previous hidden and cell state\n",
    "        dz = (np.dot(W_f.T, df)\n",
    "             + np.dot(W_i.T, di)\n",
    "             + np.dot(W_g.T, dg)\n",
    "             + np.dot(W_o.T, do))\n",
    "        dh_prev = dz[:d_hidden, :]\n",
    "        dC_prev = f[t] * dC\n",
    "        \n",
    "    grads= W_f_d, W_i_d, W_g_d, W_o_d, W_v_d, b_f_d, b_i_d, b_g_d, b_o_d, b_v_d\n",
    "    \n",
    "    # Clip gradients\n",
    "    grads = clip_gradient_norm(grads)\n",
    "    \n",
    "    return loss, grads\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a backward pass\n",
    "loss, grads = backward(z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs, targets)\n",
    "\n",
    "print('We get a loss of:')\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(params, grads, lr=1e-3):\n",
    "    # Take a step\n",
    "    for param, grad in zip(params, grads):\n",
    "        param -= lr * grad\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "\n",
    "# id data\n",
    "training_set = zip(train_x, train_y)\n",
    "validation_set = zip(test_x, test_y)\n",
    "\n",
    "# hyper-parameters\n",
    "num_epochs = 50\n",
    "\n",
    "# initialize\n",
    "d_hidden = 128\n",
    "init_lstm = my_lstm(d_hidden=d_hidden, vocab_size=vocab_size)\n",
    "param = init_lstm.params()\n",
    "\n",
    "# Track loss\n",
    "training_loss, validation_loss = [], []\n",
    "\n",
    "# For each epoch\n",
    "for i in range(num_epochs):\n",
    "    \n",
    "    # Track loss\n",
    "    epoch_training_loss = 0\n",
    "    \n",
    "    # For each sentence in training set\n",
    "    for inputs, targets in training_set:\n",
    "\n",
    "        # Initialize hidden state and cell state as zeros\n",
    "        h = np.zeros((d_hidden, 1))\n",
    "        c = np.zeros((d_hidden, 1))\n",
    "\n",
    "        # Forward pass\n",
    "        z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs = forward(inputs, h, c, params)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss, grads = backward(z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs, targets, params)\n",
    "        \n",
    "        # Update parameters\n",
    "        params = update_parameters(params, grads, lr=1e-1)\n",
    "        \n",
    "        # Update loss\n",
    "        epoch_training_loss += loss\n",
    "                \n",
    "    # Save loss for plot\n",
    "    training_loss.append(epoch_training_loss/len(training_set))\n",
    "\n",
    "    # Print loss every 5 epochs\n",
    "    if i % 5 == 0:\n",
    "        print(f'Epoch {i}, training loss: {training_loss[-1]}}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "epoch = np.arange(len(training_loss))\n",
    "plt.figure()\n",
    "plt.plot(epoch, training_loss, 'r', label='Training loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch'), plt.ylabel('NLL')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each sentence and target pair\n",
    "\n",
    "for inputs, targets in validation_set:\n",
    "\n",
    "    # Initialize hidden state and cell state as zeros\n",
    "    h = np.zeros((d_hidden, 1))\n",
    "    c = np.zeros((d_hidden, 1))\n",
    "\n",
    "    # Forward pass\n",
    "    z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs = forward(inputs, h, c, params)\n",
    "\n",
    "    # Print example\n",
    "    print('Input sentence:')\n",
    "    print(inputs)\n",
    "\n",
    "    print('\\nTarget sequence:')\n",
    "    print(targets)\n",
    "\n",
    "    print('\\nPredicted sequence:')\n",
    "    print([decoder[np.argmax(output)] for output in outputs])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
