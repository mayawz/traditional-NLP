{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/mayazwang/Files/00_Learning/_GenAI_Proj/_Learn_GenAI/traditional-NLP\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys \n",
    "import regex as re\n",
    "from collections import Counter\n",
    "\n",
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in raw text\n",
    "sys.path.append('../../data/')\n",
    "with open('data/anna.txt' , 'r') as f:\n",
    "    anna = f.read()\n",
    "print(type(anna))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up line breaks and add space around punctuation (for tokenization)\n",
    "clean_text=anna.lower().replace(\"\\n\", \" \") \n",
    "clean_text=clean_text.replace(\"-\", \" \") \n",
    "for x in \",.:;?!$()/_&%*@'`\":\n",
    "    clean_text=clean_text.replace(f\"{x}\", f\" {x} \")\n",
    "    clean_text=clean_text.replace('\"', ' \" ') \n",
    "text=clean_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', '.', 'the', '\"', 'and', 'to', 'of', 'he', \"'\", 'a']\n"
     ]
    }
   ],
   "source": [
    "# build vocab\n",
    "word_counts = Counter(text)\n",
    "vocab = sorted(word_counts, key=word_counts.get,\n",
    "reverse = True)\n",
    "print(vocab[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      ", . the\n"
     ]
    }
   ],
   "source": [
    "# definde tokenizer-encoder and tokenizer-decoder\n",
    "encoder={v:k for k,v in enumerate(vocab)} \n",
    "decoder={k:v for k,v in enumerate(vocab)}\n",
    "# test\n",
    "print(encoder['the'])\n",
    "print(decoder[0],decoder[1],decoder[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chapter', '1', 'happy', 'families', 'are', 'all', 'alike', ';', 'every', 'unhappy', 'family', 'is', 'unhappy', 'in', 'its', 'own', 'way', '.', 'everything', 'was']\n",
      "[208, 670, 283, 3024, 82, 31, 2461, 35, 202, 690, 365, 38, 690, 10, 234, 147, 166, 1, 149, 12]\n"
     ]
    }
   ],
   "source": [
    "# apply tokenizer\n",
    "tokens = [encoder[x] for x in text]\n",
    "print(text[:20])\n",
    "print(tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "440902\n",
      "308631\n",
      "132271\n"
     ]
    }
   ],
   "source": [
    "# split into train, test sets\n",
    "len_of_text = len(text)\n",
    "print(len(tokens))\n",
    "split_point = int(np.round(len_of_text*0.7))\n",
    "train_x = tokens[:split_point]\n",
    "print(len(train_x))\n",
    "test_x = tokens[split_point:]\n",
    "print(len(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creat PyTorch dataset \n",
    "context_length = 100\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# 1. Subclass torch.utils.data.Dataset\n",
    "class my_dataset(Dataset):\n",
    "    \n",
    "    # 2. Initialize with customized varargs \n",
    "    def __init__(self, tokenized_text: str, max_length: int, stride: int) -> None:\n",
    "        \n",
    "        # 3. Create class attributes\n",
    "        # initialize input_tokens_x and target_tokens_y\n",
    "        self.input_tokens_x = []\n",
    "        self.target_tokens_y = []\n",
    "        # set y as stride number of tokens trailing x \n",
    "        for i in range(0, (len(tokenized_text)-max_length), stride):\n",
    "            x_tmp = tokenized_text[i : (i+max_length)]\n",
    "            y_tmp = tokenized_text[(i+1) : (i+max_length+1)]\n",
    "            self.input_tokens_x.append(x_tmp)\n",
    "            self.target_tokens_y.append(y_tmp)\n",
    "    \n",
    "    # 4. Overwrite the __len__() method to return number of rows in the dataset\n",
    "    def __len__(self) -> int:\n",
    "        \"Returns the number of rows / pairs of x-y sequences in the dataset\"\n",
    "        return len(self.input_tokens_x)\n",
    "    \n",
    "    # 5. Overwrite the __getitem__() method (required for subclasses of torch.utils.data.Dataset)\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:\n",
    "        \"Returns one sample of data, data and label (X, y).\"\n",
    "        return self.input_tokens_x[idx], self.target_tokens_y[idx]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
