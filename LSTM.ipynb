{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM for NLP\n",
    "- predict next word in Anna Karenina\n",
    "- first with pytorch lstm\n",
    "- then with numpy lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys \n",
    "import regex as re\n",
    "from typing import Tuple, Dict, List\n",
    "from collections import Counter\n",
    "\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in raw text\n",
    "sys.path.append('../../data/')\n",
    "with open('data/anna.txt' , 'r') as f:\n",
    "    anna = f.read()\n",
    "print(type(anna))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up line breaks and add space around punctuation (for tokenization)\n",
    "clean_text=anna.lower().replace(\"\\n\", \" \") \n",
    "clean_text=clean_text.replace(\"-\", \" \") \n",
    "for x in \",.:;?!$()/_&%*@'`\":\n",
    "    clean_text=clean_text.replace(f\"{x}\", f\" {x} \")\n",
    "    clean_text=clean_text.replace('\"', ' \" ') \n",
    "text=clean_text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## implement simplified word tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocab\n",
    "word_counts = Counter(text)\n",
    "vocab = sorted(word_counts, key=word_counts.get,\n",
    "reverse = True)\n",
    "print(vocab[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definde tokenizer-encoder and tokenizer-decoder\n",
    "encoder={v:k for k,v in enumerate(vocab)} \n",
    "decoder={k:v for k,v in enumerate(vocab)}\n",
    "\n",
    "word_tokenizer = {'encoder': encoder, 'decoder': decoder}\n",
    "# save the simple tokenizer\n",
    "import joblib\n",
    "joblib.dump(word_tokenizer, 'word_tokenizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply tokenizer\n",
    "tokens = [encoder[x] for x in text]\n",
    "print(text[:20])\n",
    "print(tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train, test sets\n",
    "len_of_text = len(text)\n",
    "print(len(tokens))\n",
    "split_point = int(np.round(len_of_text*0.7))\n",
    "train_x = tokens[:split_point]\n",
    "print(len(train_x))\n",
    "test_x = tokens[split_point:]\n",
    "print(len(test_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creat PyTorch dataset \n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# 1. Subclass torch.utils.data.Dataset\n",
    "class my_dataset(Dataset):\n",
    "    \n",
    "    # 2. Initialize with customized varargs \n",
    "    def __init__(self, tokenized_text: str, max_length: int, stride: int=1) -> None:\n",
    "        \n",
    "        # 3. Create class attributes\n",
    "        # initialize input_tokens_x and target_tokens_y\n",
    "        self.input_tokens_x = [] # alternative nested tensor \n",
    "        self.target_tokens_y = []\n",
    "        # set y as stride number of tokens trailing x \n",
    "        for i in range(0, (len(tokenized_text)-max_length), stride):\n",
    "            x_tmp = tokenized_text[i : (i+max_length)]\n",
    "            y_tmp = tokenized_text[(i+1) : (i+max_length+1)]\n",
    "            self.input_tokens_x.append(torch.tensor(x_tmp))\n",
    "            self.target_tokens_y.append(torch.tensor(y_tmp))\n",
    "    \n",
    "    # 4. Overwrite the __len__() method to return number of rows in the dataset\n",
    "    def __len__(self) -> int:\n",
    "        \"Returns the number of rows / pairs of x-y sequences in the dataset\"\n",
    "        return len(self.input_tokens_x)\n",
    "    \n",
    "    # 5. Overwrite the __getitem__() method (required for subclasses of torch.utils.data.Dataset)\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:\n",
    "        \"Returns one sample of data, data and label (X, y).\"\n",
    "        return self.input_tokens_x[idx], self.target_tokens_y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and save torch dataset for later use\n",
    "context_length = 100\n",
    "dataset_train = my_dataset(tokenized_text=train_x, max_length=context_length, stride=1)\n",
    "torch.save(dataset_train, 'data/train.pt')\n",
    "dataset_test = my_dataset(tokenized_text=test_x, max_length=context_length, stride=1)\n",
    "torch.save(dataset_test, 'data/test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data with torch dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "torch.manual_seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "# num_workers = 3 \n",
    "# num_workers=num_workers # number of cpu processes to use for preprocessing\n",
    "# num_works makes next(iter()) stuck in a loop\n",
    "\n",
    "loader_train = DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True, # if True, # drops the last batch if it is shorter than the specified batch_size to prevent loss spikes during training  \n",
    ")\n",
    "\n",
    "print(type(loader_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the next pair in dataset\n",
    "x,y=next(iter(loader_train))\n",
    "print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(encoder) # len(vocab) # \n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class word_lstm(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size, \n",
    "                 emb_dim,\n",
    "                 lstm_layers,\n",
    "                 drop_out_rate):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_dim = emb_dim\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.drop_out_rate = drop_out_rate\n",
    "\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.emb_dim)\n",
    "        self.lstm = nn.LSTM(input_size=self.emb_dim,\n",
    "                            hidden_size=self.emb_dim,\n",
    "                            num_layers=self.lstm_layers,\n",
    "                            dropout=self.drop_out_rate,\n",
    "                            batch_first=True)\n",
    "        self.fc = nn.Linear(self.emb_dim, self.vocab_size, bias=True)\n",
    "\n",
    "    def forward(self, x, hc):\n",
    "        embed = self.embedding(x)\n",
    "        x, hc = self.lstm(embed, hc)\n",
    "        x = self.fc(x)\n",
    "        return x, hc\n",
    "    \n",
    "    def init_hidden(self, context_length):\n",
    "        weight = next(self.parameters()).data\n",
    "        return (weight.new(self.lstm_layers, context_length, self.emb_dim).zero_(),\n",
    "                weight.new(self.lstm_layers, context_length, self.emb_dim).zero_())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word_lstm(vocab_size=vocab_size,\n",
    "                  emb_dim=128,\n",
    "                  lstm_layers=3,\n",
    "                  drop_out_rate=0.2).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.0001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "for epoch in range(50):\n",
    "    tloss=0\n",
    "    sh,sc = model.init_hidden(batch_size)\n",
    "    for i, (x,y) in enumerate(loader_train):    \n",
    "        if x.shape[0]==batch_size:\n",
    "            inputs, targets = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output, (sh,sc) = model(inputs, (sh,sc))\n",
    "            loss = loss_func(output.transpose(1,2),targets)\n",
    "            sh,sc=sh.detach(),sc.detach()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "            optimizer.step()\n",
    "            tloss+=loss.item()\n",
    "        if (i+1)%100==0:\n",
    "            print(f\"at epoch {epoch} iteration {i+1}\\\n",
    "            average loss = {tloss/(i+1)}\")\n",
    "\n",
    "# at epoch 0 iteration 100            average loss = 6.056614637374878\n",
    "# at epoch 0 iteration 500            average loss = 6.038397843360901\n",
    "# at epoch 0 iteration 1200            average loss = 6.0286973142623905"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\"model/word_lstm).pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference - greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference - top-k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## numpy LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
